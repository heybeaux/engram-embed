[package]
name = "engram-embed"
version = "0.1.0"
edition = "2021"
description = "Local embedding server - drop-in replacement for OpenAI embeddings"
authors = ["Beaux Walton <beaux@heybeaux.com>"]
license = "MIT"

[dependencies]
# Web server
axum = "0.7"
tokio = { version = "1", features = ["full"] }
tower = "0.4"
tower-http = { version = "0.5", features = ["cors", "trace"] }

# ML runtime (Candle - HuggingFace's Rust ML framework)
candle-core = { version = "0.9", features = ["metal"] }
candle-nn = "0.9"
candle-transformers = "0.9"

# Tokenization
tokenizers = "0.20"

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Utilities
anyhow = "1"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }
hf-hub = "0.4"  # Download models from HuggingFace

# System info (memory usage)
libc = "0.2"

# Metal acceleration (macOS) - enabled via candle-core feature
# Note: Metal support is built into candle-core with the "metal" feature

[dev-dependencies]
reqwest = { version = "0.11", features = ["json"] }
tokio-test = "0.4"

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
