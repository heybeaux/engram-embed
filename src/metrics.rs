//! In-memory metrics collection for engram-embed
//!
//! Provides lightweight metrics without external dependencies:
//! - Total embeddings count
//! - Per-model embedding counts
//! - Per-model latency tracking (rolling average)
//! - Memory usage
//! - Uptime tracking

use serde::Serialize;
use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::RwLock;
use std::time::Instant;

/// Rolling average tracker for latency
#[derive(Debug)]
pub struct LatencyTracker {
    /// Sum of all latencies in microseconds
    total_us: AtomicU64,
    /// Count of samples
    count: AtomicU64,
    /// Min latency in microseconds
    min_us: AtomicU64,
    /// Max latency in microseconds
    max_us: AtomicU64,
}

impl Default for LatencyTracker {
    fn default() -> Self {
        Self {
            total_us: AtomicU64::new(0),
            count: AtomicU64::new(0),
            min_us: AtomicU64::new(u64::MAX),
            max_us: AtomicU64::new(0),
        }
    }
}

impl LatencyTracker {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn record(&self, latency_us: u64) {
        self.total_us.fetch_add(latency_us, Ordering::Relaxed);
        self.count.fetch_add(1, Ordering::Relaxed);
        
        // Update min (CAS loop) - start with u64::MAX so first value always wins
        loop {
            let current_min = self.min_us.load(Ordering::Relaxed);
            if latency_us >= current_min {
                break;
            }
            if self.min_us.compare_exchange_weak(
                current_min,
                latency_us,
                Ordering::Relaxed,
                Ordering::Relaxed,
            ).is_ok() {
                break;
            }
        }

        // Update max (CAS loop)
        loop {
            let current_max = self.max_us.load(Ordering::Relaxed);
            if latency_us <= current_max {
                break;
            }
            if self.max_us.compare_exchange_weak(
                current_max,
                latency_us,
                Ordering::Relaxed,
                Ordering::Relaxed,
            ).is_ok() {
                break;
            }
        }
    }

    pub fn avg_ms(&self) -> f64 {
        let count = self.count.load(Ordering::Relaxed);
        if count == 0 {
            return 0.0;
        }
        let total = self.total_us.load(Ordering::Relaxed);
        (total as f64 / count as f64) / 1000.0
    }

    pub fn min_ms(&self) -> Option<f64> {
        let min = self.min_us.load(Ordering::Relaxed);
        if min == u64::MAX {
            None
        } else {
            Some(min as f64 / 1000.0)
        }
    }

    pub fn max_ms(&self) -> Option<f64> {
        let max = self.max_us.load(Ordering::Relaxed);
        if max == 0 {
            None
        } else {
            Some(max as f64 / 1000.0)
        }
    }

    pub fn count(&self) -> u64 {
        self.count.load(Ordering::Relaxed)
    }
}

/// Per-model metrics
#[derive(Debug, Default)]
pub struct ModelMetrics {
    /// Total embeddings generated by this model
    pub embeddings_count: AtomicU64,
    /// Total texts embedded (a single request can embed multiple texts)
    pub texts_count: AtomicU64,
    /// Latency tracking
    pub latency: LatencyTracker,
}

impl ModelMetrics {
    pub fn new() -> Self {
        Self::default()
    }
}

/// Global metrics state
pub struct Metrics {
    /// Server start time
    start_time: Instant,
    /// Total embedding requests across all models
    total_requests: AtomicU64,
    /// Total texts embedded across all models
    total_texts: AtomicU64,
    /// Per-model metrics
    per_model: RwLock<HashMap<String, ModelMetrics>>,
    /// Last embedding timestamp (unix epoch seconds)
    last_embedding_time: AtomicU64,
    /// Currently in-flight requests
    in_flight: AtomicUsize,
}

impl Metrics {
    pub fn new() -> Self {
        Self {
            start_time: Instant::now(),
            total_requests: AtomicU64::new(0),
            total_texts: AtomicU64::new(0),
            per_model: RwLock::new(HashMap::new()),
            last_embedding_time: AtomicU64::new(0),
            in_flight: AtomicUsize::new(0),
        }
    }

    /// Record start of a request (returns guard that decrements on drop)
    pub fn request_start(&self) -> InFlightGuard<'_> {
        self.in_flight.fetch_add(1, Ordering::Relaxed);
        InFlightGuard { metrics: self }
    }

    /// Record a completed embedding request
    pub fn record_embedding(&self, model: &str, text_count: usize, latency_us: u64) {
        // Update totals
        self.total_requests.fetch_add(1, Ordering::Relaxed);
        self.total_texts.fetch_add(text_count as u64, Ordering::Relaxed);

        // Update last embedding time
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        self.last_embedding_time.store(now, Ordering::Relaxed);

        // Update per-model metrics
        {
            let models = self.per_model.read().unwrap();
            if let Some(m) = models.get(model) {
                m.embeddings_count.fetch_add(1, Ordering::Relaxed);
                m.texts_count.fetch_add(text_count as u64, Ordering::Relaxed);
                m.latency.record(latency_us);
                return;
            }
        }

        // Model not found, need to insert
        let mut models = self.per_model.write().unwrap();
        let m = models.entry(model.to_string()).or_insert_with(ModelMetrics::new);
        m.embeddings_count.fetch_add(1, Ordering::Relaxed);
        m.texts_count.fetch_add(text_count as u64, Ordering::Relaxed);
        m.latency.record(latency_us);
    }

    /// Get uptime in seconds
    pub fn uptime_secs(&self) -> u64 {
        self.start_time.elapsed().as_secs()
    }

    /// Get last embedding timestamp (unix epoch seconds, 0 if never)
    pub fn last_embedding_time(&self) -> u64 {
        self.last_embedding_time.load(Ordering::Relaxed)
    }

    /// Get number of in-flight requests
    pub fn in_flight_count(&self) -> usize {
        self.in_flight.load(Ordering::Relaxed)
    }

    /// Get total request count
    pub fn total_requests(&self) -> u64 {
        self.total_requests.load(Ordering::Relaxed)
    }

    /// Get total texts embedded
    pub fn total_texts(&self) -> u64 {
        self.total_texts.load(Ordering::Relaxed)
    }

    /// Get memory usage in bytes (RSS on supported platforms)
    pub fn memory_bytes(&self) -> u64 {
        get_memory_usage()
    }

    /// Export metrics as a snapshot
    pub fn snapshot(&self) -> MetricsSnapshot {
        let per_model = self.per_model.read().unwrap();
        let model_stats: HashMap<String, ModelStats> = per_model
            .iter()
            .map(|(name, m)| {
                (
                    name.clone(),
                    ModelStats {
                        requests: m.embeddings_count.load(Ordering::Relaxed),
                        texts: m.texts_count.load(Ordering::Relaxed),
                        latency_avg_ms: m.latency.avg_ms(),
                        latency_min_ms: m.latency.min_ms(),
                        latency_max_ms: m.latency.max_ms(),
                    },
                )
            })
            .collect();

        MetricsSnapshot {
            uptime_secs: self.uptime_secs(),
            total_requests: self.total_requests(),
            total_texts: self.total_texts(),
            in_flight: self.in_flight_count(),
            memory_bytes: self.memory_bytes(),
            last_embedding_time: self.last_embedding_time(),
            per_model: model_stats,
        }
    }
}

impl Default for Metrics {
    fn default() -> Self {
        Self::new()
    }
}

/// Guard that decrements in-flight count on drop
pub struct InFlightGuard<'a> {
    metrics: &'a Metrics,
}

impl Drop for InFlightGuard<'_> {
    fn drop(&mut self) {
        self.metrics.in_flight.fetch_sub(1, Ordering::Relaxed);
    }
}

/// Serializable metrics snapshot
#[derive(Debug, Serialize)]
pub struct MetricsSnapshot {
    pub uptime_secs: u64,
    pub total_requests: u64,
    pub total_texts: u64,
    pub in_flight: usize,
    pub memory_bytes: u64,
    pub last_embedding_time: u64,
    pub per_model: HashMap<String, ModelStats>,
}

#[derive(Debug, Serialize)]
pub struct ModelStats {
    pub requests: u64,
    pub texts: u64,
    pub latency_avg_ms: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub latency_min_ms: Option<f64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub latency_max_ms: Option<f64>,
}

/// Get current process memory usage (RSS) in bytes
#[cfg(target_os = "macos")]
fn get_memory_usage() -> u64 {
    use std::mem::MaybeUninit;
    
    // Use mach APIs for accurate RSS on macOS
    unsafe {
        let mut info: MaybeUninit<libc::rusage> = MaybeUninit::uninit();
        if libc::getrusage(libc::RUSAGE_SELF, info.as_mut_ptr()) == 0 {
            let info = info.assume_init();
            // maxrss is in bytes on macOS (unlike Linux where it's KB)
            info.ru_maxrss as u64
        } else {
            0
        }
    }
}

#[cfg(target_os = "linux")]
fn get_memory_usage() -> u64 {
    // Read from /proc/self/statm for RSS
    if let Ok(statm) = std::fs::read_to_string("/proc/self/statm") {
        let parts: Vec<&str> = statm.split_whitespace().collect();
        if parts.len() >= 2 {
            if let Ok(rss_pages) = parts[1].parse::<u64>() {
                // Page size is typically 4096
                return rss_pages * 4096;
            }
        }
    }
    0
}

#[cfg(not(any(target_os = "macos", target_os = "linux")))]
fn get_memory_usage() -> u64 {
    0 // Unsupported platform
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_latency_tracker() {
        let tracker = LatencyTracker::new();
        
        tracker.record(1000); // 1ms
        tracker.record(2000); // 2ms
        tracker.record(3000); // 3ms
        
        assert_eq!(tracker.count(), 3);
        assert!((tracker.avg_ms() - 2.0).abs() < 0.01);
        assert_eq!(tracker.min_ms(), Some(1.0));
        assert_eq!(tracker.max_ms(), Some(3.0));
    }

    #[test]
    fn test_metrics_recording() {
        let metrics = Metrics::new();
        
        metrics.record_embedding("bge-base", 2, 5000);
        metrics.record_embedding("bge-base", 1, 3000);
        metrics.record_embedding("minilm", 1, 2000);
        
        assert_eq!(metrics.total_requests(), 3);
        assert_eq!(metrics.total_texts(), 4);
        
        let snapshot = metrics.snapshot();
        assert_eq!(snapshot.per_model.len(), 2);
        assert_eq!(snapshot.per_model["bge-base"].requests, 2);
        assert_eq!(snapshot.per_model["bge-base"].texts, 3);
        assert_eq!(snapshot.per_model["minilm"].requests, 1);
    }

    #[test]
    fn test_in_flight_guard() {
        let metrics = Metrics::new();
        
        assert_eq!(metrics.in_flight_count(), 0);
        
        {
            let _guard = metrics.request_start();
            assert_eq!(metrics.in_flight_count(), 1);
            
            let _guard2 = metrics.request_start();
            assert_eq!(metrics.in_flight_count(), 2);
        }
        
        assert_eq!(metrics.in_flight_count(), 0);
    }
}
